{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#importing neccessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost.sklearn import XGBClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sessions=pd.read_csv('../input/airbnb-recruiting-new-user-bookings/sessions.csv.zip')\ncountries=pd.read_csv('../input/airbnb-recruiting-new-user-bookings/countries.csv.zip')\nage_gender=pd.read_csv('../input/airbnb-recruiting-new-user-bookings/age_gender_bkts.csv.zip')\nsubmission=pd.read_csv('../input/airbnb-recruiting-new-user-bookings/sample_submission_NDF.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the Data\ntrain= pd.read_csv('../input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip')\ntest = pd.read_csv('../input/airbnb-recruiting-new-user-bookings/test_users.csv.zip')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.signup_method.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.affiliate_channel.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Arama Motoru Optimizasyonu (SEO), Web sitelerinin arama motorlarında daha iyi performans göstermesi için yapılan çalışmaların tümüne verilen isimdir.\n\n* Remarketing en yalın tanımı ile, sitenizi ziyaret eden kişilerin etiketlenerek, sitenizden ayrıldıktan sonra ziyaret ettikleri diğer sitelerde tekrar reklamınızı görmelerini sağlamaktır."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.affiliate_provider.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.first_affiliate_tracked.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.first_device_type.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.first_browser.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.signup_app.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries.sort_values(by='distance_km')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=train['country_destination'].value_counts() \na=a.drop(['NDF','other'],axis=0) \ndf_value_counts = pd.DataFrame(a)\ndf_value_counts = a.reset_index() \ndf_value_counts.columns = ['country_destination', 'value_counts'] # change column names \nmapdata= pd.merge(df_value_counts, countries,how= 'inner' , on='country_destination')\nmapdata['poppercent']=mapdata['value_counts']/mapdata['value_counts'].sum() \nmapdata['text']=['United States', 'France', 'Italy','United Kingdom', 'Spanish','Canada' ,'German', 'Dutch','Australia', 'Brazil' ] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a world map to show distributions of users \nimport folium\nfrom folium.plugins import MarkerCluster\n#empty map\nworld_map= folium.Map(tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(world_map)\n#for each coordinate, create circlemarker of user percent\nfor i in range(len(mapdata)):\n        lat = mapdata.iloc[i]['lat_destination']\n        long = mapdata.iloc[i]['lng_destination']\n        radius=5\n        popup_text = \"\"\"Country : {}<br>\n                    %of Users : {}<br>\"\"\"\n        popup_text = popup_text.format(mapdata.iloc[i]['country_destination'],\n                                   mapdata.iloc[i]['poppercent']\n                                   )\n        folium.CircleMarker(location = [lat, long], radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\n#show the map\nworld_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Australya(AU),İngiltere(GB),Kanada(CA) ve Amerika(US) ingilizce konuşuyor. İnsanlar dillerinin konuşulduğu yerleri öncelikli olarak tercih etmiş olabilir mi? Yoksa Amerikaya veya Amerikaya yakın yerlerin tercih edilmesi daha olası mı?"},{"metadata":{"trusted":true},"cell_type":"code","source":"a=train.groupby('country_destination').count().sort_values(by='id',ascending=True)\na.id\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"İnsanlar öncelikli olarak Amerika içinde seyehat etmişler. En çok seyehat edilen(adını bildiğimiz) ikinci ülke Fransa, üçüncü ülke ise İtalya. Yani dil ve uzaklık tercih edilme açısından bir kriterdir diyemeyiz. Language_levenshtein_distance ile country destination arasındaki ilişkiyi gözlemleyebileceğimiz bir plot çizerek bu hipotez kontrol edilebilir. Ama yorumlarımda bir değişiklik olmayacaktır."},{"metadata":{"trusted":true},"cell_type":"code","source":"countries.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_gender.drop(age_gender.index[0],inplace=True)\nage_gender","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_gender.country_destination.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_gender.age_bucket.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agebtw0n19= age_gender[(age_gender.age_bucket=='0-4') | (age_gender.age_bucket=='5-9') | (age_gender.age_bucket=='10-14')|(age_gender.age_bucket=='15-19')]\nagebtw49n34= age_gender[(age_gender.age_bucket=='45-49') | (age_gender.age_bucket=='40-44') | (age_gender.age_bucket=='35-39') | (age_gender.age_bucket=='30-34')]\nagebtw99n50= age_gender[(age_gender.age_bucket=='95-99') | (age_gender.age_bucket=='90-94') | (age_gender.age_bucket=='85-89') | (age_gender.age_bucket=='80-84') | (age_gender.age_bucket=='75-79') | (age_gender.age_bucket=='70-74') | (age_gender.age_bucket=='70-74') | (age_gender.age_bucket=='65-69') | (age_gender.age_bucket=='60-64') | (age_gender.age_bucket=='55-59') | (age_gender.age_bucket=='50-54') ]\nagebtw20n29= age_gender[(age_gender.age_bucket=='25-29') | (age_gender.age_bucket=='20-24')]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nax = sns.boxplot(x=agebtw20n29[\"population_in_thousands\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nax = sns.boxplot(x=agebtw99n50[\"population_in_thousands\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Box plot kodunun üzerinden kendi oluşturdugum yaş grupları için ayrı ayrı box plotlar çizdirdim. Ancak yaş ve gidilecek yerin nufusu arasında bir ilişki gözlemleyemedim. Yaşlılar daha sakin yerleri, gençler daha kalabalık yerleri tercih edebilir gibi bir durum gözlemlerim diye bir varsayımda bulunmuştum. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nimport seaborn as sns\nage_gender.sort_values(\"age_bucket\", ascending=False,inplace=True)\ncolors=sns.color_palette()\nsns.stripplot(x=\"age_bucket\",y=\"population_in_thousands\",data=age_gender,jitter=True,hue='country_destination',palette='pastel')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"60 yaşından büyükler yaşları ilerledikçe daha sakin lokasyonları tercih etmiş. Ama genel olarak yaş, gidilen ülke ve populasyon büyüklüğü arasındadoğrudan bir ilişki yok gibi görünüyor. Ama yıl bilgisi sadece 2015 yılına ait. Chi squre test katagorik değişkenlerimiz arasında bir ilişki olup olmadığını test etmemizi sağlayacaktır."},{"metadata":{"trusted":true},"cell_type":"code","source":"actionanditsdetail=pd.crosstab(age_gender.age_bucket, age_gender.country_destination,margins=False)\nfrom scipy.stats import chi2_contingency \n\n# defining the table \n\nstat, p, dof, expected = chi2_contingency(actionanditsdetail) \n\n# interpret p-value \nalpha = 0.05\nprint(\"p value is \" + str(p)) \nif p <= alpha: \n\tprint('Dependent (reject H0)') \nelse: \n\tprint('Independent (H0 holds true)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actionanditsdetail=pd.crosstab(age_gender.country_destination, age_gender.gender,margins=False)\nfrom scipy.stats import chi2_contingency \n\n# defining the table \n\nstat, p, dof, expected = chi2_contingency(actionanditsdetail) \n\n# interpret p-value \nalpha = 0.05\nprint(\"p value is \" + str(p)) \nif p <= alpha: \n\tprint('Dependent (reject H0)') \nelse: \n\tprint('Independent (H0 holds true)')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"age_gender ve countries datalarını incelediğimde train datasıyla birleştirebileceğim bir değişken gözlemleyemedim. İnsanların tatillerini geçirebileceği yer seçimlerinde gidilecek ülkenin uzaklığı, o ülkede konuşulan dilin ingilizce veya ingilizceye yakın bir dil ailesine sahip oluşu,cinsiyet, kişinin yaşı ve yaşa bağlı olarak gidilen yerin nufus yogunluğu arasında anlamlı bir ilişki gözlemleyemedim. Ama tatile gidilecek döneme bağlı olarak insanlar şehir dışına çıkmayı veya Amerika içinde gezmeyi tercih etmiş olabilirler."},{"metadata":{"trusted":true},"cell_type":"code","source":"sessions=sessions.sort_values('user_id')\nsessions.rename(columns={\"user_id\": \"id\"},inplace=True)\nsessions.sort_values(by='id',inplace=True)\nsessions = sessions[(sessions['secs_elapsed'].notnull()) & (sessions['secs_elapsed'] > 0.0) ]  \nsessions[(sessions['action_detail']=='booking')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sadece 3 kişinin hareketleri booking ile sonuçlanmış. 3 ü de windows kullanıcısı olarak görünüyor ama bence eylemlerin rezervasyon ile sonuçlanması detaylı araştırma gerektireceği için bilgisayardan islem yapılıyor olması daha olası. Daha sonra device type ı bilgisayar olup olmamasına göre yeniden isimlendirebilirim. train datasına sitede geçirilen toplam süre bilgisi ekleyip bu veriyi scale edebilirim. Bu iki yöntemin modelimi iyileştirip iyileştirmediğine bakabilirim."},{"metadata":{"trusted":true},"cell_type":"code","source":"groupedid=pd.DataFrame(sessions.groupby(['id'])['secs_elapsed'].sum())\ngroupedid['hour_elapsed']=groupedid['secs_elapsed'].div(3600).round(decimals=0)\ngroupedid.drop('secs_elapsed',axis=1,inplace=True)\ngroupedid.reset_index(inplace=True)\ngroupedid[(groupedid['id'] == '6udv3scuxe') | (groupedid['id'] == 'yxf0sm9sbw') | (groupedid['id'] == 'nttj7g9av6')] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groupedid.hour_elapsed.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rezervasyon yapan insanların sitede geçirdikleri toplam süre için diger insanların geçirdikleri sürenin %75 inden fazla zaman geçirdikleri söylenebilir. Süre bakımından üst çeyreklikte bulunuyorlar."},{"metadata":{"trusted":true},"cell_type":"code","source":"actionndevicetype=pd.crosstab(sessions.action, sessions.device_type,margins=False)\nfrom scipy.stats import chi2_contingency \n\n# defining the table \n\nstat, p, dof, expected = chi2_contingency(actionndevicetype) \n\n# interpret p-value \nalpha = 0.05\nprint(\"p value is \" + str(p)) \nif p <= alpha: \n\tprint('Dependent (reject H0)') \nelse: \n\tprint('Independent (H0 holds true)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Device type ve action arasında bir ilişki var gibi görünüyor. % 95 güven aralığıyla kullanıcıların hareketleriyle kullandıkları alet arasında anlamlı bir ilişki vardır diyebiliriz."},{"metadata":{},"cell_type":"markdown","source":"Train datasıyla birleştirebileceğimherhengi bir özellik gözlemleyemedim. Ancak modellerimin doğrulugunu arttırmak için train datasının kolonlarıyla oynayıp datayı yeniden şekillendirebileceğimi düşündüm."},{"metadata":{},"cell_type":"markdown","source":"# TRAIN DATA EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(train.country_destination.value_counts())\nsns.countplot(train.country_destination)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sınıflar eşit dağılmamış. Azınlık sınıfların kötü tahmin edilmesine sebep olacak bir problem. 'resampling' teknikleri deneyerek bir denge sağlayabileceğimi düşünüyorum."},{"metadata":{},"cell_type":"markdown","source":"# MODELLING"},{"metadata":{},"cell_type":"markdown","source":"1. MISSINGNESS"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n#Loading the Data again\ntrain= pd.read_csv('../input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip')\ntest = pd.read_csv('../input/airbnb-recruiting-new-user-bookings/test_users.csv.zip')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train ve test i yenilemek istediğimde notebookun başına gitmemek için buraya taşıdım bu kodları."},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['first_affiliate_tracked'] = train['first_affiliate_tracked'].fillna('Unknown')\ntest['first_affiliate_tracked'] = test['first_affiliate_tracked'].fillna('Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.boxplot(x=train.age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n#train=train[(train.age > 14) & (train.age < 110)]# modele sadece 14 yaşından büyük ve 110 yaşından küçükleri dahil edebilirim.\n#test=test[(test.age > 14) & (test.age < 110)]\ntrain[(train.age < 14) & (train.age > 110)]=np.nan\ntest[(test.age < 14) & (test.age > 110)]=np.nan\ntrain['age'].fillna(train['age'].mean(), inplace=True)\ntest['age'].fillna(train['age'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"14 yaşından küçüklerin aileleriyle seyehat edeceklerini varsayarak 14 yaşından küçükleri modele dahil etmedim. Önceki araştırmalarımda küçük yaştaki yolcuların tercihleriyle büyük yaştaki yolcuların tercihleri arasında anlamlı bir fark olmadığını gördüm. Ayrıca 110 yaşından büyük insanların yaşlarını incelediğimde anlamsız yaşlarda(2014 gibi) insanlar gördüm. O nedenle sadece 110 yaşından küçük ve 14 yaşından büyük insanlar için sınıflandırma yapmaya karar verdim."},{"metadata":{},"cell_type":"markdown","source":"Mean imputation yerine başka yöntemler de denenebilirdi. Ama age değişkeni mean imputaion için uygun bir değişkendi. Ayrıca pratik bir yöntem olduğunu düşündüm. Modeli iyileştirip iyileştirmeyeceğini görmek için başka imputation yöntemleri de denenebilir. 14 yaşından küçük çocukları dataya dahil ederek çocuklu ebebynlerin seyehat tercihleri hakkında bir fikir sahibi olunabilirdi belki ama age_gender datasına baktıgımda yaş gurupları ve country destinationları arasında bir ilişki gözlemleyememiştim. 14 yaşından küçükleri modele dahil etmek modelim üzerinde büyük farklaryaratmayacaktır. Ek olarak, 110 yaşından büyük yaşları incelediğimde 2014, 1929 gibi yaşlar gözlemledim. Bunlar missing gibi görünmesede aslında kayıp bilgiler. 800 küşür kişiye ait bilgileri modelden atmaktansa onları da missing olarak kabul edip imputation uygulamak modellerimi iyileştirebilir. Sonraki aşamalarda modelimi ileriye taşımak adına deneyeceğim bir yöntem olacaktır."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting below columns as categories for plotting in graphs\ncategorical_features = [\n    'affiliate_channel',\n    'affiliate_provider',\n    'first_affiliate_tracked',\n    'first_browser',\n    'first_device_type',\n    'gender',\n    'language',\n    'signup_app',\n    'signup_method',\n    'signup_flow'\n]\n\nfor categorical_feature in categorical_features:\n    train[categorical_feature] = train[categorical_feature].astype('category')\nfor categorical_feature in categorical_features:\n    test[categorical_feature] = test[categorical_feature].astype('category')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"signup_flow değişkeni sayısal bir değermiş gibi dursa da önceki notebooklarımdan birinde grafiksel olarak bu değişkene kategorik demenin daha doğru olacağını düşünmüştüm. Yarışmaya dair açıklamaların yazdığı sayfada bunun siteye kayıt olunmadan önceki sayfa olduguna dair bir açıklama vardı. Bu iki durumu göz önünde bulundurunca signup_flow değişkenine kategorik gibi davranmam gerektiğine karar verdim."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date_account_created'] = pd.to_datetime(train['date_account_created'])\ntrain['date_first_booking'] = pd.to_datetime(train['date_first_booking'])\ntrain['timestamp_first_active'] = pd.to_datetime(train['timestamp_first_active'], format='%Y%m%d%H%M%S')\ntrain['timestamp_first_active'] = pd.to_datetime(train['timestamp_first_active']).dt.date\ntest['date_account_created'] = pd.to_datetime(test['date_account_created'])\ntest['timestamp_first_active'] = pd.to_datetime(test['timestamp_first_active'], format='%Y%m%d%H%M%S')\ntest['timestamp_first_active'] = pd.to_datetime(test['timestamp_first_active']).dt.date\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# Use seaborn style defaults and set the default figure size\n\ndf = train.groupby(['date_first_booking'])['country_destination'].count().reset_index()\ndf.dropna(axis=0,inplace=True)\nimport plotly.express as px\n\nfig = px.line(df, x='date_first_booking', y=\"country_destination\")\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Her yıl temmuz ve agustos aylarında ilk rezervasyon sayıları artmış. 2011'den 2014' e kadar genel bir artış var. 2104 yılının temmuz ayından mayıs 2015' e kadar azalışa geçmiş. 2015'in temmuz ve agustos aylarına dair bir bilgi mevcut değil."},{"metadata":{},"cell_type":"markdown","source":"Rezervasyonlar üzerinde zaman etkisi gözardı edilemez bu nedenle tarih bilgisi bulunduran verileri sınıflandırma algoritmasına dahil etmek modellerimi iyileştirebilir."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['date_first_booking'], axis=1,inplace=True)\ntest.drop(['date_first_booking'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#date_account_created\n\ndac = np.vstack(train.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ntrain['dac_year'] = dac[:,0]\ntrain['dac_month'] = dac[:,1]\ntrain['dac_day'] = dac[:,2]\ntrain.drop(['date_account_created'], axis=1,inplace=True)\n#timestamp_first_active\ntfa = np.vstack(train.timestamp_first_active.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ntrain['tfa_year'] = tfa[:,0]\ntrain['tfa_month'] = tfa[:,1]\ntrain['tfa_day'] = tfa[:,2]\ntrain.drop(['timestamp_first_active'], axis=1,inplace=True)\n#date_account_created\n\ndac = np.vstack(test.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ntest['dac_year'] = dac[:,0]\ntest['dac_month'] = dac[:,1]\ntest['dac_day'] = dac[:,2]\ntest.drop(['date_account_created'], axis=1,inplace=True)\n#timestamp_first_active\ntfa = np.vstack(test.timestamp_first_active.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ntest['tfa_year'] = tfa[:,0]\ntest['tfa_month'] = tfa[:,1]\ntest['tfa_day'] = tfa[:,2]\ntest.drop(['timestamp_first_active'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sitede gerçekleştirilen ilk eylemin zamanının ve siteye ilk kayıt olunan tarihin zamanlarının gün, ay, yıl bilgilerini kullanarak yeni değişkenler üretmek modelimi iyileştirse de çok büyük farklar yaratmadı. Sürekli yeni yöntemler denediğim için bu bilgilerin modelden atılmış halini sizlerle paylaşmadım. Ayrıca date_first_booking değişkenini veriden attım. Çünkü zaten ilk booking yapılan zaman test datasında yok. Aslında country_destination'ı tahmin etmeden önce date_first_booking' i de tahmin etmek gerekiyor olabilir. Çünkü yeni bir müşterinin ne zaman geleceği, geldiğinde gitmek isteyebileceği ilk yeri etkileyen bir faktör. Yani burada aslında zamana bağlı/dönemsel bir takım etkiler de söz konusu. Aynı anda birden fazla değişkeni tahminleyen yöntem ya da önce yeni müşterilerin gelecekleri tarihleri tahminleyip sonra 'gelseler, nereye giderlerdi?' sorusunu sormak mantıklı olabilir belki."},{"metadata":{},"cell_type":"markdown","source":"# Computation for the Booking Destination"},{"metadata":{},"cell_type":"markdown","source":"# LABEL ENCODING"},{"metadata":{},"cell_type":"markdown","source":"Sınıflandırma algoritmaları kullanacağım için one-hot encoding yerine label encoding kullanmayı tercih ettim. Eğer bu veri regresyona uygun olsaydı one-hot encoding uygulardım. Çünkü label encoding yaptıgımda bir değişkene ait sınıf çok fazla olduğunda bir sınıfın diğerinden daha önemli olduğunu varsayabiliyor modelimiz. Sayısal bir büyüklük olarak algılıyor sınıflara verilen sayıları. Bu da modellerimizin doğruluğunu olumsuz etkileyebilir. Ancak sınıflandırma algoritmalarında böyle bir sorun söz konusu değil. One-hot-encoding uygulamak da yalnış olmazdı ancak şimdilik buna gerek olduğunu düşünmüyorum."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \nle = LabelEncoder() \n  \ntrain['gender']= le.fit_transform(train['gender'])\ntrain['signup_method']= le.fit_transform(train['signup_method']) \ntrain['first_affiliate_tracked']= le.fit_transform(train['first_affiliate_tracked']) \ntrain['signup_method']= le.fit_transform(train['signup_method']) \ntrain['language']= le.fit_transform(train['language'])\ntrain['affiliate_channel']= le.fit_transform(train['affiliate_channel'])\ntrain['affiliate_provider']= le.fit_transform(train['affiliate_provider'])\ntrain['signup_app']= le.fit_transform(train['signup_app'])\ntrain['first_device_type']= le.fit_transform(train['first_device_type'])\ntrain['first_browser']= le.fit_transform(train['first_browser'])\ntrain['signup_flow']= le.fit_transform(train['signup_flow'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder() \n  \ntest['gender']= le.fit_transform(test['gender'])\ntest['signup_method']= le.fit_transform(test['signup_method']) \ntest['first_affiliate_tracked']= le.fit_transform(test['first_affiliate_tracked']) \ntest['signup_method']= le.fit_transform(test['signup_method']) \ntest['language']= le.fit_transform(test['language'])\ntest['affiliate_channel']= le.fit_transform(test['affiliate_channel'])\ntest['affiliate_provider']= le.fit_transform(test['affiliate_provider'])\ntest['signup_app']= le.fit_transform(test['signup_app'])\ntest['first_device_type']= le.fit_transform(test['first_device_type'])\ntest['first_browser']= le.fit_transform(test['first_browser'])\ntest['signup_flow']= le.fit_transform(test['signup_flow'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.country_destination.replace('NDF',0,inplace=True)\ntrain.country_destination.replace('US',1,inplace=True)\ntrain.country_destination.replace('other',2,inplace=True)\ntrain.country_destination.replace('FR',3,inplace=True)\ntrain.country_destination.replace('CA',4,inplace=True)\ntrain.country_destination.replace('GB',5,inplace=True)\ntrain.country_destination.replace('ES',6,inplace=True)\ntrain.country_destination.replace('IT',7,inplace=True)\ntrain.country_destination.replace('PT',8,inplace=True)\ntrain.country_destination.replace('NL',9,inplace=True)\ntrain.country_destination.replace('DE',10,inplace=True)\ntrain.country_destination.replace('AU',11,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny=train['country_destination']\nX=train.drop(['country_destination','id'],axis=1)\nfrom imblearn.combine import SMOTETomek\n# transform the dataset\nsmotetomek = SMOTETomek(sampling_strategy='auto')\n\n# split the dataset into train and test sets\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.30, random_state=1,shuffle=True,stratify=y)\nX_train, y_train = smotetomek.fit_resample(X_train1, y_train1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SMOTETomek- resampling yaparak sınıfları dengeliyorum. Train-test ayrımını 'stratify=y' ile yaptığımda  'Stratified Samplimg' diğer adıyla tabakalı örnekleme yaparak country_destination değişkenini temsil eden her bir alt sınıfı train ve test verilerine dahik ettiğime ein oluyorum."},{"metadata":{},"cell_type":"markdown","source":"> Using Stratified Sampling technique ensures that there will be selection from each sub-groups and prevents the chance of omitting one sub-group leading to sampling bias"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pd.DataFrame(X_train)\nY=pd.DataFrame(y_train)\nresult = pd.concat([x, Y], axis=1, join='inner')\nsns.countplot(result.country_destination)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Resampling yapmak sınıfları daha eşit dağıttı. "},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = ['NDF', 'US', 'other', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU']\n#Classifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nrf=RandomForestClassifier()\n\nrf.fit(X_train,y_train)\n\ny_predrf=rf.predict(X_test1)\nprint(classification_report(y_test1, y_predrf, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names=list(X_train1.columns.values.tolist()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfeature_imp = pd.Series(rf.feature_importances_,index=feature_names).sort_values(ascending=False)\nfeature_imp\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**** 0.69 submission score (smoote oversampling kullandığımda).\n    Farklı resampling algoritmalarını deneyerek modelimi en iyi haline getirmeye çalışasagım.\nSmootetomek fonsiyonu scorumu biraz düşürsede classlarımı çok güzel dengeledi. Bu nedenle modelim için SMOTE'dan daha iyi bir seçenek olduğunu düşünüyorum."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_country={0:\"NDF\", 1:\"US\", 2:\"other\", 3:\"FR\", 4:\"CA\", 5:\"GB\", 6:\"ES\", 7:\"IT\", 8:\"PT\", 9:\"DE\", 10:\"NL\", 11:\"AU\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier\nxgb = XGBClassifier()                  \n\nxgb.fit(X_train,y_train)\n\n\ny_predxgb=xgb.predict(X_test1)\n\nprint(classification_report(y_test1, y_predxgb, target_names=target_names))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest a kıyasla daha iyi bir classification raporum olduğunu söyleyebilirim. Resampling uyguladıktan ve XGBoost yaptıktan sonra submission scorum 0.59 a düştü. Ancak Başlangıçta country_destination değişkenin içindeki veriler eşit örneğe sahip değildi. Bu nedenle veride en sık bulunan iki sınıfı çok iyi tahmin ederken diğer sınıfları neredeyse hiç tahmin etmiyordu modelim. Bu problemi çözmek için 'resampling' yöntemlerini araştırdım. Undersampling yöntemlerini deneyecek kadar büyük bir veri setine sahip olmadığım için oversampling yöntemlerine yöneldim. Ancak bu yöntemleri denedikten sonra bile 'balanced' bir y değişkenine sahip olamadığımı gözlemledim. Sonra over ve under sampling yöntemlerinin kombinasyonu olarak tanımlanabilecek yöntemlere yöneldim. Sınıflarımı en iyi dengeleyen yöntem [SMOTEomek](http://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine) oldu. "},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsxgb=xgb.predict(test.drop(['id'],axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultsxgb=[]\nfor i in predictionsxgb:\n    resultsxgb.append(pred_country[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#my_submissionxgb = pd.DataFrame({'id': test.id, 'country':resultsxgb})\n#my_submissionxgb.to_csv('submissionxgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n#Generate prediction using Neural Net\n\nmlp = MLPClassifier(activation='identity', solver='sgd',learning_rate='adaptive', alpha=0.0001, batch_size='auto')\nmlp.fit(X_train,y_train)\npredsmlp = mlp.predict(X_test1)\nfrom sklearn import metrics\nprint(classification_report(y_test1, predsmlp, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mlp accuracy olarak XGB den daha yüksek bir accuray ye sahip olsa da azınlık sınıfları tahminlerken çuvalladı. Rezervasyon yapmayacak kullanıcıları iyi tahmin etti ancak yapanların nereye gideceğini hiç tahmin edemedi. Submission scorumsa 0.67'ye yükseldi. Bazı parametreleri değiştirmenin modelimin üzerinde nasıl bir etki bırakacağını gözlemlemek istiyorum."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'solver': ['lbfgs','sgd'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\nmlpgridsearch = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\nmlpgridsearch.fit(X_train,y_train)\npredsgridmlp = mlpgridsearch.predict(X_test1)\nfrom sklearn import metrics\nprint(classification_report(y_test1, predsgridmlp, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import ComplementNB cnb = ComplementNB() cnb.fit(X_train, y_train) y_predcnb=cnb.predict(X_test1)\n\nfrom sklearn import metrics\n\nprint(classification_report(y_test1, y_predcnb, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tablo değerleri random forest  ve mlp ile kıyasladığımda baya kötü. Ancak tüm sınıflara (destinationlara) yapılacak ilk rezervasyonları mlp'den daha iyi tahmin etti. Random Forest ı geçemediği için bu algoritmayı eledim."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd import numpy as np from sklearn.preprocessing import LabelEncoder from xgboost.sklearn import XGBClassifier\n\n#Loading the Data again train= pd.read_csv('../input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip') test = pd.read_csv('../input/airbnb-recruiting-new-user-bookings/test_users.csv.zip') train['first_affiliate_tracked'] = train['first_affiliate_tracked'].fillna('Unknown') test['first_affiliate_tracked'] = test['first_affiliate_tracked'].fillna('Unknown') train['date_account_created'] = pd.to_datetime(train['date_account_created']) train['timestamp_first_active'] = pd.to_datetime(train['timestamp_first_active'], format='%Y%m%d%H%M%S') train['timestamp_first_active'] = pd.to_datetime(train['timestamp_first_active']).dt.date test['date_account_created'] = pd.to_datetime(test['date_account_created']) test['timestamp_first_active'] = pd.to_datetime(test['timestamp_first_active'], format='%Y%m%d%H%M%S') test['timestamp_first_active'] = pd.to_datetime(test['timestamp_first_active']).dt.date train.drop(['date_first_booking'], axis=1,inplace=True) test.drop(['date_first_booking'], axis=1,inplace=True)\n\n#date_account_created\n\ndac = np.vstack(train.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) train['dac_year'] = dac[:,0] train['dac_month'] = dac[:,1] train['dac_day'] = dac[:,2] train.drop(['date_account_created'], axis=1,inplace=True)\n\n#timestamp_first_active tfa = np.vstack(train.timestamp_first_active.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) train['tfa_year'] = tfa[:,0] train['tfa_month'] = tfa[:,1] train['tfa_day'] = tfa[:,2] train.drop(['timestamp_first_active'], axis=1,inplace=True)\n\n#date_account_created\n\ndac = np.vstack(test.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) test['dac_year'] = dac[:,0] test['dac_month'] = dac[:,1] test['dac_day'] = dac[:,2] test.drop(['date_account_created'], axis=1,inplace=True)\n\n#timestamp_first_active tfa = np.vstack(test.timestamp_first_active.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) test['tfa_year'] = tfa[:,0] test['tfa_month'] = tfa[:,1] test['tfa_day'] = tfa[:,2] test.drop(['timestamp_first_active'], axis=1,inplace=True) import numpy as np train[(train.age < 14) & (train.age > 110)]=np.nan train['age'].fillna(train['age'].mean(), inplace=True)\n\ntest[(test.age < 14) & (test.age > 110)]=np.nan test['age'].fillna(train['age'].mean(), inplace=True)\n\n#Converting below columns as categories for plotting in graphs categorical_features = [ 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'first_browser', 'first_device_type', 'gender', 'language', 'signup_app', 'signup_method', 'signup_flow' ]\n\nfor categorical_feature in categorical_features: train[categorical_feature] = train[categorical_feature].astype('category') for categorical_feature in categorical_features: test[categorical_feature] = test[categorical_feature].astype('category')\n\n#One-hot-encoding features ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'] for f in ohe_feats: train_dummy = pd.get_dummies(train[f], prefix=f) train_cont= train.drop([f], axis=1) train = pd.concat((train_cont, train_dummy), axis=1)\n\n#One-hot-encoding features ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'] for f in ohe_feats: test_dummy = pd.get_dummies(test[f], prefix=f) test_cont= test.drop([f], axis=1) test = pd.concat((test_cont, test_dummy), axis=1)\n\n#Splitting train and test\n\nfrom sklearn.model_selection import train_test_split y=train['country_destination'] X=train.drop(['country_destination','id'],axis=1) from imblearn.combine import SMOTETomek\n\ntransform the dataset\nsmotetomek = SMOTETomek(sampling_strategy='auto')\n\nsplit the dataset into train and test sets\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.30, random_state=1,shuffle=True) X_train, y_train = smotetomek.fit_resample(X_train1, y_train1)\n\n#Classifier\n\nxgb = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints=None, learning_rate=0.3, max_delta_step=0, max_depth=6, min_child_weight=1, monotone_constraints=None, n_estimators=25, n_jobs=0, num_parallel_tree=1, objective='multi:softprob', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None, seed=0, subsample=0.5, tree_method=None, validate_parameters=False, verbosity=None) xgb.fit(X_train, y_train) target_names = ['NDF', 'US', 'other', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU'] y_predxgb=xgb.predict(X_test1)\n\nfrom sklearn.metrics import classification_report print(classification_report(y_test1, y_predxgb, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sadece İtalya destinationının presicion değeri yüksek çıktı. Farklı parametler ve one hot encoding kullanarak yeniden XGboost denediğimde accuracy 0.2 arttı ama bu model pek doğru sınıflandırma yapmadı."},{"metadata":{},"cell_type":"markdown","source":"#Eklemek istediklerim:"},{"metadata":{},"cell_type":"markdown","source":"Önceki denemelerimde timestamp_first_active ve date_account_created değişkenlerini üzerlerinde hiçbir değişiklik yapmadan silmiştim. Ama bu iki değişkeni bölüp gün,ay,yıl şeklinde yeni değişkenler üretmek submission scorumu neredeyse hiç değiştirmedi. En yüksek scorelarımı sadece date_first_booking değişkenini kullandığımda elde etmiştim. Bu değişkeni gün,ay,yıl olacak şekilde 3 e parçalayıp veri setine eklemiştin ve ardından kategorik değişkenlerime one-hot-endoding uygulamıştım, scorelarım çok anlamlı bir şekilde değişmişti ancak 'Test' verisinde date_first_booking değişkeni tamamen boştu ayrıca azınlıkta olan sınıfların tahmin edilebilirliği oldukça düşüktü hatta bazı sınıflar hiç tahmin edilemiyordu. Bu nedenle yarışmaya herhangi bir yükleme yapamadım. Yeni kullanıcıların ilk rezervasyon lokasyonları zamana bağlı değişebilen bir şey. Belki[ bu dataya uygun bir time series ](https://otexts.com/fpp2/hierarchical.html)algoritması bulunabilir ve dataya o şekilde yaklaşılabilir."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}